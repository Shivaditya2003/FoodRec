{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b968cd-5e70-4e28-941c-1de42331d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84255bd5-2489-4988-8a91-dd7660df560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [19:55:12] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 53.92%\n",
      "K-Nearest Neighbors Accuracy: 38.24%\n",
      "SVM Accuracy: 54.90%\n",
      "Random Forest Accuracy: 35.29%\n",
      "XGBoost Accuracy: 40.20%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "data = pd.read_csv('synthetic_dish_data.csv')\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Dropping duplicates\n",
    "data_cleaned = data.drop_duplicates()\n",
    "\n",
    "# Dropping rows with missing critical values\n",
    "data_cleaned = data_cleaned.dropna(subset=['Dish', 'Dieting_Level', 'Spice_Level', 'Time_of_Day', 'Age_Group', 'Preferred_Cuisine'])\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data_cleaned['Dieting_Level'] = label_encoder.fit_transform(data_cleaned['Dieting_Level'])\n",
    "data_cleaned['Spice_Level'] = label_encoder.fit_transform(data_cleaned['Spice_Level'])\n",
    "data_cleaned['Time_of_Day'] = label_encoder.fit_transform(data_cleaned['Time_of_Day'])\n",
    "data_cleaned['Age_Group'] = label_encoder.fit_transform(data_cleaned['Age_Group'])\n",
    "data_cleaned['Preferred_Cuisine'] = label_encoder.fit_transform(data_cleaned['Preferred_Cuisine'])\n",
    "data_cleaned['Dish'] = label_encoder.fit_transform(data_cleaned['Dish'])\n",
    "\n",
    "# Step 4: Splitting data into features (X) and target (y)\n",
    "X = data_cleaned[['Dieting_Level', 'Spice_Level', 'Time_of_Day', 'Age_Group', 'Preferred_Cuisine']]\n",
    "y = data_cleaned['Dish']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Testing Multiple Classifiers\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Support Vector Classifier (SVM)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print the accuracies of each model\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_logistic * 100:.2f}%\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn * 100:.2f}%\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef558d8-68fa-464e-ac18-390b01c648ef",
   "metadata": {},
   "source": [
    "### The accuracy results you're getting indicate that the models might not be learning the patterns effectively from the current feature set, or the dataset might need further refinement. Here are a few steps you can take to improve the model performance:\n",
    "\n",
    "#### 1. Feature Engineering\n",
    "Add more relevant features or refine existing ones. For example, you could create new features based on existing data, such as interaction terms (combinations of Dieting_Level and Spice_Level).\n",
    "Normalize or scale features if they vary widely in range.\n",
    "#### 2. Handling Imbalanced Data\n",
    "If the target variable (Dish) is imbalanced, some models may perform poorly. Use techniques like oversampling, undersampling, or SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.\n",
    "#### 3. Model Hyperparameter Tuning\n",
    "Use techniques like GridSearchCV or RandomizedSearchCV to optimize hyperparameters of each model. For instance:\n",
    "K-Nearest Neighbors: Tune the number of neighbors.\n",
    "SVM: Tune the regularization parameter C and kernel type.\n",
    "Random Forest: Tune the number of estimators and max depth.\n",
    "XGBoost: Tune learning rate, depth, and estimators.\n",
    "#### 4. Cross-Validation\n",
    "Implement cross-validation to get a more robust estimate of the model’s performance and reduce variance due to random splits in train/test data.\n",
    "#### 5. Examine Feature Importance\n",
    "For models like Random Forest and XGBoost, you can plot the feature importance to see which features are most impactful. You may also try removing less important features.\n",
    "#### 6. Data Augmentation or Enrichment\n",
    "If you feel the dataset might be lacking, consider gathering more data or performing data augmentation if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4eaef6-dc97-41a1-b59b-a2da1c03b5f2",
   "metadata": {},
   "source": [
    "### The dataset contains 1,000 entries with the following six columns, all of which are categorical:\n",
    "\n",
    "Dish – Target variable (the type of dish).\n",
    "Dieting_Level – Level of dieting (Low, Medium, High).\n",
    "Spice_Level – Spice preference (Low, Medium, Spicy).\n",
    "Time_of_Day – Time of meal (Breakfast, Lunch, Dinner).\n",
    "Age_Group – Age group of the person (Child, Adult, Senior).\n",
    "Preferred_Cuisine – Preferred cuisine (Indian, Chinese, Italian, etc.).\n",
    "Let's proceed with the following steps:\n",
    "\n",
    "Feature Engineering: Explore interactions between features and scaling if necessary.\n",
    "Handling Imbalanced Data: Check for imbalance in the target (Dish) and consider techniques if needed.\n",
    "Hyperparameter Tuning: Implement GridSearchCV for model tuning.\n",
    "Cross-Validation: Use cross-validation for a better performance estimate.\n",
    "Feature Importance: Analyze the importance of features using Random Forest and XGBoost.\n",
    "Data Augmentation/Enrichment: Explore ways to increase dataset richness if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace312ea-1ddf-4f43-aa59-4768c6de0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms\n",
    "data['Diet_Spice_Interaction'] = data['Dieting_Level'] + '_' + data['Spice_Level']\n",
    "data['Time_Cuisine_Interaction'] = data['Time_of_Day'] + '_' + data['Preferred_Cuisine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714a0680-83f8-43ee-9b46-5106e3043c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encoding categorical features\n",
    "data_encoded = data.copy()\n",
    "categorical_columns = ['Dieting_Level', 'Spice_Level', 'Time_of_Day', 'Age_Group', 'Preferred_Cuisine', 'Diet_Spice_Interaction', 'Time_Cuisine_Interaction']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    data_encoded[col] = label_encoder.fit_transform(data_encoded[col])\n",
    "\n",
    "# Splitting data into features and target variable\n",
    "X = data_encoded.drop(columns=['Dish'])\n",
    "y = label_encoder.fit_transform(data_encoded['Dish'])\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b833f884-ac53-45c8-8c87-0430329525ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593ebde1-4e7c-4294-b258-a82cbf2dd0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: [0.63461538 0.62019231 0.68269231 0.67307692 0.71875   ]\n",
      "Mean accuracy: 0.6658653846153847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Example for Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "cv_scores = cross_val_score(logistic_model, X_resampled, y_resampled, cv=5)\n",
    "\n",
    "print(f\"Cross-validated scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a560f6d2-0b6f-4876-a067-49ee81cd0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time_Cuisine_Interaction: 0.3471\n",
      "Preferred_Cuisine: 0.2736\n",
      "Time_of_Day: 0.1468\n",
      "Age_Group: 0.0849\n",
      "Diet_Spice_Interaction: 0.0751\n",
      "Spice_Level: 0.0430\n",
      "Dieting_Level: 0.0294\n"
     ]
    }
   ],
   "source": [
    "# Random Forest feature importance\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_resampled.columns\n",
    "sorted_importances = sorted(zip(importances, feature_names), reverse=True)\n",
    "\n",
    "# Display feature importance\n",
    "for importance, name in sorted_importances:\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df4fd2-022f-4fec-bfbd-874732eb5e01",
   "metadata": {},
   "source": [
    "### Train and Evaluate Models\n",
    "Now that your data is balanced, you can train and evaluate different machine learning models to compare their performance. Below is a guide to proceed:\n",
    "\n",
    "Split the Data: You should split the resampled dataset into training and testing sets.\n",
    "Train Multiple Classifiers: Use models like Logistic Regression, K-Nearest Neighbors, Random Forest, SVM, and XGBoost.\n",
    "Evaluate Performance: Compare the accuracy of each model.\n",
    "Here’s the code to implement these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477c5191-63f7-4718-85b4-85c71b4e7ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/arshad-ali/anaconda3/envs/virtualenv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [19:56:12] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 67.79%\n",
      "K-Nearest Neighbors Accuracy: 69.71%\n",
      "SVM Accuracy: 64.18%\n",
      "Random Forest Accuracy: 71.88%\n",
      "XGBoost Accuracy: 73.80%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Step 1: Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Train and evaluate multiple classifiers\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Support Vector Classifier (SVM)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Step 3: Print the accuracies of each model\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_logistic * 100:.2f}%\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn * 100:.2f}%\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cafaabe-f0a3-4602-ba43-01bba4ade1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Sample 1: Frittata\n",
      "Prediction for Sample 2: Burger\n",
      "Prediction for Sample 3: Tacos\n"
     ]
    }
   ],
   "source": [
    "# Sample 1 with 7 features (Dieting_Level, Spice_Level, Time_of_Day, Age_Group, Preferred_Cuisine, Diet_Spice_Interaction, Time_Cuisine_Interaction)\n",
    "sample_1 = [[1, 2, 1, 0, 3, 2, 3]]\n",
    "\n",
    "# Sample 2 with 7 features\n",
    "sample_2 = [[0, 3, 2, 1, 0, 3, 1]]\n",
    "\n",
    "# Sample 3 with 7 features\n",
    "sample_3 = [[2, 1, 0, 2, 4, 1, 2]]\n",
    "\n",
    "# Assuming you have the original LabelEncoder instance for 'Dish'\n",
    "# If you've already defined it as 'label_encoder' for 'Dish', you can use that\n",
    "\n",
    "# Predictions\n",
    "pred_1 = xgb_model.predict(sample_1)\n",
    "pred_2 = xgb_model.predict(sample_2)\n",
    "pred_3 = xgb_model.predict(sample_3)\n",
    "\n",
    "# Convert the encoded labels back to dish names\n",
    "dish_name_1 = label_encoder.inverse_transform(pred_1)\n",
    "dish_name_2 = label_encoder.inverse_transform(pred_2)\n",
    "dish_name_3 = label_encoder.inverse_transform(pred_3)\n",
    "\n",
    "# Output the dish names\n",
    "print(f\"Prediction for Sample 1: {dish_name_1[0]}\")\n",
    "print(f\"Prediction for Sample 2: {dish_name_2[0]}\")\n",
    "print(f\"Prediction for Sample 3: {dish_name_3[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea84de3-98cf-415b-b377-e66e8c509599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
